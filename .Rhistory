nrow(sea_data) == n_distinct(sea_data$sid, sea_data$sch_g7_lea_id)
sea_data$sid <- paste(sea_data$sid, sea_data$sch_g7_lea_id, sep = "-")
nrow(sea_data) == n_distinct(sea_data$sid)
table(sea_data$year)
table(sea_data$cohort_year)
table(sea_data$cohort_grad_year)
table(sea_data$year_of_graduation == sea_data$cohort_grad_year)
table(sea_data$ontime_grad)
table(sea_data$year_of_graduation <= sea_data$cohort_grad_year + 1)
length(unique(sea_data$first_hs_name))
length(unique(sea_data$first_hs_lea_id))
table(sea_data$coop_name_g7, useNA = "always")
my_coop <- sea_data$coop_name_g7[50] # select the coop for the 50th observation
# Which districts are in this coop and how many 7th graders do we have for each?
table(sea_data$sch_g7_lea_id[sea_data$coop_name_g7 == my_coop],
useNA = "always")
# which schools?
table(sea_data$sch_g7_name[sea_data$coop_name_g7 == my_coop],
useNA = "always")
table(sea_data$male, useNA="always")
table(sea_data$ontime_grad, useNA = "always")
table(sea_data$transferout, useNA = "always")
table(transfer = sea_data$transferout, grad = sea_data$ontime_grad, useNA = "always")
summary(sea_data$vendor_ews_score)
set_thresh <- 0.5
conf_count <- table(observed = sea_data$ontime_grad,
pred = sea_data$vendor_ews_score > set_thresh)
conf_count
set_thresh <- mean(sea_data$vendor_ews_score)
conf_count <- table(observed = sea_data$ontime_grad,
pred = sea_data$vendor_ews_score > set_thresh)
conf_count
# Create a proportion table and round for easier interpretation
round(
prop.table(conf_count),
digits = 3
)
summary(sea_data$scale_score_7_math)
hist(sea_data$scale_score_7_math)
# TODO: Replace this with a visualization
by(sea_data$scale_score_7_math, sea_data$coop_name_g7, FUN = mean,
na.rm = TRUE)
by(sea_data$scale_score_7_math, sea_data$frpl_7, FUN = mean,
na.rm = TRUE)
effect_size_diff(sea_data$scale_score_7_math, sea_data$ontime_grad,
na.rm = TRUE)
effect_size_diff(sea_data$pct_days_absent_7, sea_data$ontime_grad,
na.rm = TRUE)
sea_data$pct_absent_cat <- round(sea_data$pct_days_absent_7, digits = 0)
table(sea_data$pct_absent_cat)
sea_data$pct_absent_cat[sea_data$pct_absenct_cat >= 30] <- 30
# TODO - Update this plot sequence to ggplot2
sea_data <- sea_data %>%
group_by(pct_absent_cat) %>% # perform the operation for each value
mutate(abs_ontime_grad = mean(ontime_grad, na.rm = TRUE)) %>% # add a new variable
as.data.frame()
plot(sea_data$pct_absent_cat, sea_data$abs_ontime_grad)
hist(sea_data$scale_score_7_math)
sea_data$scale_score_7_math[sea_data$scale_score_7_math < 0] <- NA
hist(sea_data$scale_score_7_math)
sea_data <- sea_data %>%
mutate(math_7_cut = ntile(scale_score_7_math, n = 100)) %>%
group_by(math_7_cut) %>% # perform the operation for each value
mutate(math_7_ontime_grad = mean(ontime_grad, na.rm=TRUE)) %>% # add a new variable
as.data.frame()
plot(sea_data$math_7_cut, sea_data$math_7_ontime_grad)
for(var in c("coop_name_g7", "male", "race_ethnicity")){
print(var)
print(
prop.table(table(sea_data[, var],
"missing_math" = is.na(sea_data$pct_days_absent_7)), 1)
)
}
hist(sea_data$pct_days_absent_7)
sea_data$pct_days_absent_7[sea_data$pct_days_absent_7 > 100] <- NA
hist(sea_data$pct_days_absent_7)
# TODO - explain missing data handling here? # 1354 observations are deleted
math_model <- glm(ontime_grad ~ scale_score_7_math, data = sea_data,
family = "binomial") # family tells R we want to fit a logistic
summary(math_model)
logit_rsquared(math_model)
math_model2 <- glm(ontime_grad ~ scale_score_7_math +
I(scale_score_7_math^2) + I(scale_score_7_math^3),
data = sea_data,
family = "binomial") # family tells R we want to fit a logistic
logit_rsquared(math_model2)
absence_model <- glm(ontime_grad ~ pct_days_absent_7, data = sea_data,
family = "binomial")
summary(absence_model)
logit_rsquared(absence_model)
combined_model <- glm(ontime_grad ~ pct_days_absent_7 + scale_score_7_math,
data = sea_data, family = "binomial")
summary(combined_model)
logit_rsquared(combined_model)
sea_data$grad_pred <- predict(combined_model, newdata = sea_data,
type = "response") # this tells R to give us a probability
table(is.na(sea_data$grad_pred))
basic_thresh <- mean(sea_data$ontime_grad)
basic_thresh
sea_data$grad_indicator <- ifelse(sea_data$grad_pred > basic_thresh, 1, 0)
table(sea_data$grad_indicator, useNA = "always")
sea_data <- sea_data %>%
mutate(grad_pred_cut = ntile(grad_pred, n = 100)) %>%
group_by(grad_pred_cut) %>% # perform the operation for each value
mutate(grad_pred_cut_grad = mean(ontime_grad, na.rm=TRUE)) # add a new variable
plot(sea_data$grad_pred_cut, sea_data$grad_pred_cut_grad)
prop.table(
table(
grad = sea_data$ontime_grad,
pred = sea_data$grad_indicator)) %>% # shorthand way to round
round(3)
new_thresh <- basic_thresh - 0.05
prop.table(table(Observed = sea_data$ontime_grad,
Predicted = sea_data$grad_pred > new_thresh)) %>%
round(3)
prop.table(table(Observed = sea_data$ontime_grad,
Predicted = sea_data$grad_pred > new_thresh,
useNA = "always")) %>% round(3)
table(Grad = sea_data$ontime_grad,
miss_math = is.na(sea_data$scale_score_7_math)) %>%
prop.table(2) %>%  # get proportions by columns
round(3) # round
table(Grad = sea_data$ontime_grad,
miss_abs = is.na(sea_data$pct_days_absent_7)) %>%
prop.table(2) %>% # get proportions by columns
round(3) # round
absence_model <- glm(ontime_grad ~ pct_days_absent_7,
data = sea_data[is.na(sea_data$scale_score_7_math),],
family = "binomial")
sea_data$grad_pred_2 <- predict(absence_model, newdata = sea_data,
type = "response")
summary(absence_model)
table(sea_data$grad_indicator, useNA="always")
sea_data$grad_indicator[is.na(sea_data$grad_pred) &
sea_data$grad_pred_2 < new_thresh] <- 0
sea_data$grad_indicator[is.na(sea_data$grad_pred) &
sea_data$grad_pred_2 >= new_thresh] <- 1
table(sea_data$grad_indicator, useNA="always")
table(sea_data$grad_indicator, sea_data$ontime_grad, useNA = "always")
sea_data$grad_indicator[is.na(sea_data$grad_indicator)] <- 0
table(Observed = sea_data$ontime_grad, Predicted = sea_data$grad_indicator) %>%
prop.table %>% round(3)
table(Observed = sea_data$ontime_grad, Predicted = sea_data$grad_indicator) %>%
prop.table(margin = 1) %>% round(3)
roc(sea_data$ontime_grad, sea_data$grad_indicator) %>% plot
roc(sea_data$ontime_grad, sea_data$grad_pred) %>% plot
library(caret)
# We must wrap these each in calls to factor because of how this function expects
# the data to be formatted
caret::confusionMatrix(factor(sea_data$grad_indicator),
factor(sea_data$ontime_grad), positive = "1")
# In R we can define an index of rows so we do not have to copy our data
train_idx <- row.names(sea_data[sea_data$year %in% c(2003, 2004, 2005),])
test_idx <- !row.names(sea_data) %in% train_idx
fit_model <- glm(ontime_grad ~ scale_score_7_math + frpl_7,
data = sea_data[train_idx, ], family = "binomial")
sea_data$grad_pred_3 <- predict(fit_model, newdata = sea_data, type = "response")
summary(sea_data$grad_pred_3)
# Check the test index only
summary(sea_data$grad_pred_3[test_idx])
# calculate matrix of the test_index
table(Observed = sea_data$ontime_grad[test_idx],
Predicted = sea_data$grad_pred_3[test_idx] > new_thresh) %>%
prop.table() %>% round(4)
library(caret) # machine learning workhorse in R
sea_data <- as.data.frame(sea_data)
# To use caret we have to manually expand our categorical variables. caret
# can use formulas like lm() or glm() but becomes very slow and unreliable
# for some model/algorithm methods when doing so. Better to do it ourselves.
# Expand categorical variables
# First declare any variable we want to be treated as a category as a factor
sea_data$frpl_7 <- factor(sea_data$frpl_7)
sea_data$male <- factor(sea_data$male)
# Use the dummyVars function to build an expansion function to expand our data
expand_factors <- caret::dummyVars(~ race_ethnicity + frpl_7 + male,
data = sea_data)
# Create a matrix of indicator variables by using the predict method for the
# expand_factors object we just created. There are other ways you can do this, but
# the advantage is we can use the `expand_factors` object on future/different
# datasets to preserve factor levels that may not be present in those (like our
# test set perhaps)
fact_vars <- predict(expand_factors, sea_data)
# Get the column names of our categorical dummy variables
categ_vars <- colnames(fact_vars)
# Combine the new dummy variables with our original dataset
sea_data <- cbind(sea_data, fact_vars)
# Drop the matrix of dummy variables
rm(fact_vars)
# Define the numeric/continuous predictors we want to use
continuous_x_vars <- c('scale_score_7_math', 'scale_score_7_read',
'pct_days_absent_7', 'sch_g7_frpl_per')
# caret does not gracefully handle missing data, so we have to do some additional
# work when we define our training/test data split. You can choose additional
# ways to address missing data (imputation, substituting mean values, etc.) -
# here we opt for the simple but aggressive strategy of excluding any row
# with missing values for any predictor from being in the training data
train_idx <- row.names( # get the row.names to define our index
na.omit(sea_data[sea_data$year %in% c(2003, 2004, 2005),
# reduce the data to rows of specific years and not missing data
c(continuous_x_vars, categ_vars)])
# only select the predictors we are interested in
)
test_idx <- !row.names(sea_data[sea_data$year > 2005,]) %in% train_idx
# All of our algorithms will run much faster and more smoothly if we center
# our continuous measures at 0 with a standard deviation of 1. We have
# skipped the important step of identifying and removing outliers, which we
# should make sure to do, but is left up to you!
pre_proc <- caret::preProcess(sea_data[train_idx, continuous_x_vars],
method = c("scale", "center"))
# We now have a pre-processing object which will scale and center our variables
# for us. It will ignore any variables that are not defined within it, so we
# can pass all of our continuous and dummy variables to it to produce our
# final data frame of predictors.
preds <- predict(pre_proc,
sea_data[train_idx, # keep only the training rows
c(continuous_x_vars, categ_vars)]
) # keep only the columns of dummy and continuous variables
# Caret really really really likes if you do binary classification that you
# code the variables as factors with alphabetical labels. In this case, we
# recode 0/1 to be nongrad, grad.
yvar <- sea_data[train_idx, "ontime_grad"] # save only training observations
yvar <- ifelse(yvar == 1, "grad", "nongrad")
yvar <- factor(yvar)
# On a standard desktop/laptop it can be necessary to decrease the sample size
# to train in a reasonable amount of time. For the prototype and getting feedback
# it's a good idea to stick with a reasonable sample size of under 30,000 rows.
# Let's do that here:
train_idx_small <- sample(1:nrow(preds), 2e4)
# Caret has a number of complex options, you can read about under ?trainControl
# Here we set some sensible defaults
example_control <- trainControl(
method = "cv", # we cross-validate our model to avoid overfitting
classProbs = TRUE,  # we want to be able to predict probabilities, not just binary outcomes
returnData = TRUE, # we want to store the model data to allow for postestimation
summaryFunction = prSummary, # we want to use the prSummary for better two-class accuracy measures
trim = TRUE, # we want to reduce the size of the final model object to save RAM
savePredictions = "final", # we want to store the predictions from the final model
returnResamp = "final", # we want to store the resampling code to directly compare other methods
allowParallel = TRUE # we want to use all the processors on our computer if possible
)
# This will take quite some time:
set.seed(2532) # set seed so models are comparable
rpart_model <- train(
y = yvar[train_idx_small], # specify the outcome, here subset
# to the smaller number of observations
x = preds[train_idx_small, 2:16], # specify the matrix of predictors, again subset
method = "rpart",  # choose the algorithm - here a regression tree
tuneLength = 24, # choose how many values of tuning parameters to test
trControl = example_control, # set up the conditions for the test (defined above)
metric = "AUC" # select the metric to choose the best model based on
)
class(yvar)
table(yvar)
# This will take quite some time:
set.seed(2532) # set seed so models are comparable
rpart_model <- train(
y = yvar[train_idx_small], # specify the outcome, here subset
# to the smaller number of observations
x = preds[train_idx_small, ], # specify the matrix of predictors, again subset
method = "rpart",  # choose the algorithm - here a regression tree
tuneLength = 24, # choose how many values of tuning parameters to test
trControl = example_control, # set up the conditions for the test (defined above)
metric = "AUC" # select the metric to choose the best model based on
)
View(example_control)
rpart_model <- train(
y = yvar[train_idx_small], # specify the outcome, here subset
# to the smaller number of observations
x = preds[train_idx_small, ], # specify the matrix of predictors, again subset
method = "glm",  # choose the algorithm - here a regression tree
tuneLength = 24, # choose how many values of tuning parameters to test
trControl = example_control, # set up the conditions for the test (defined above)
metric = "AUC" # select the metric to choose the best model based on
)
# Caret has a number of complex options, you can read about under ?trainControl
# Here we set some sensible defaults
example_control <- trainControl(
method = "cv", # we cross-validate our model to avoid overfitting
classProbs = TRUE,  # we want to be able to predict probabilities, not just binary outcomes
returnData = TRUE, # we want to store the model data to allow for postestimation
summaryFunction = twoClassSummary, # we want to use the prSummary for better two-class accuracy measures
trim = TRUE, # we want to reduce the size of the final model object to save RAM
savePredictions = "final", # we want to store the predictions from the final model
returnResamp = "final", # we want to store the resampling code to directly compare other methods
allowParallel = TRUE # we want to use all the processors on our computer if possible
)
set.seed(2532) # set seed so models are comparable
rpart_model <- train(
y = yvar[train_idx_small], # specify the outcome, here subset
# to the smaller number of observations
x = preds[train_idx_small, ], # specify the matrix of predictors, again subset
method = "rpart",  # choose the algorithm - here a regression tree
tuneLength = 24, # choose how many values of tuning parameters to test
trControl = example_control, # set up the conditions for the test (defined above)
metric = "AUC" # select the metric to choose the best model based on
)
# Set options for knitr
library(knitr)
knitr::opts_chunk$set(comment = NA, warning = FALSE, echo = TRUE,
root.dir = normalizePath("../"),
error = FALSE, message = FALSE, fig.align = 'center',
fig.width = 8, fig.height = 6, dpi = 144,
fig.path = "../figure/pa_2",
cache.path = "../cache/pa_2")
options(width = 80)
# Load the packages you need
library(dplyr)
library(pROC)
library(devtools)
library(future)
library(klaR)
# Load the helper functions not in packages
source("../R/functions.R")
# Read in the data
# This command assumes that the data is in a folder called data, below your
# current working directory. You can check your working directory with the
# getwd() command, and you can set your working directory using the RStudio
# environment, or the setwd() command.
load("../data/montucky.rda")
# RESUME PREPPING DATA FROM GUIDE 1
sea_data$sid <- paste(sea_data$sid, sea_data$sch_g7_lea_id, sep = "-")
sea_data$scale_score_7_math[sea_data$scale_score_7_math < 0] <- NA
library(caret) # machine learning workhorse in R
sea_data <- as.data.frame(sea_data)
# To use caret we have to manually expand our categorical variables. caret
# can use formulas like lm() or glm() but becomes very slow and unreliable
# for some model/algorithm methods when doing so. Better to do it ourselves.
# Expand categorical variables
# First declare any variable we want to be treated as a category as a factor
sea_data$frpl_7 <- factor(sea_data$frpl_7)
sea_data$male <- factor(sea_data$male)
# Use the dummyVars function to build an expansion function to expand our data
expand_factors <- caret::dummyVars(~ race_ethnicity + frpl_7 + male,
data = sea_data)
# Create a matrix of indicator variables by using the predict method for the
# expand_factors object we just created. There are other ways you can do this, but
# the advantage is we can use the `expand_factors` object on future/different
# datasets to preserve factor levels that may not be present in those (like our
# test set perhaps)
fact_vars <- predict(expand_factors, sea_data)
# Get the column names of our categorical dummy variables
categ_vars <- colnames(fact_vars)
# Combine the new dummy variables with our original dataset
sea_data <- cbind(sea_data, fact_vars)
# Drop the matrix of dummy variables
rm(fact_vars)
# Define the numeric/continuous predictors we want to use
continuous_x_vars <- c('scale_score_7_math', 'scale_score_7_read',
'pct_days_absent_7', 'sch_g7_frpl_per')
# caret does not gracefully handle missing data, so we have to do some additional
# work when we define our training/test data split. You can choose additional
# ways to address missing data (imputation, substituting mean values, etc.) -
# here we opt for the simple but aggressive strategy of excluding any row
# with missing values for any predictor from being in the training data
train_idx <- row.names( # get the row.names to define our index
na.omit(sea_data[sea_data$year %in% c(2003, 2004, 2005),
# reduce the data to rows of specific years and not missing data
c(continuous_x_vars, categ_vars)])
# only select the predictors we are interested in
)
test_idx <- !row.names(sea_data[sea_data$year > 2005,]) %in% train_idx
# All of our algorithms will run much faster and more smoothly if we center
# our continuous measures at 0 with a standard deviation of 1. We have
# skipped the important step of identifying and removing outliers, which we
# should make sure to do, but is left up to you!
pre_proc <- caret::preProcess(sea_data[train_idx, continuous_x_vars],
method = c("scale", "center"))
# We now have a pre-processing object which will scale and center our variables
# for us. It will ignore any variables that are not defined within it, so we
# can pass all of our continuous and dummy variables to it to produce our
# final data frame of predictors.
preds <- predict(pre_proc,
sea_data[train_idx, # keep only the training rows
c(continuous_x_vars, categ_vars)]
) # keep only the columns of dummy and continuous variables
# Take advantage of all the processing power on your machine
library(doFuture)
plan(multiprocess(workers = 4)) # define the number of cpus to use
registerDoFuture() # register them with R
# Caret really really really likes if you do binary classification that you
# code the variables as factors with alphabetical labels. In this case, we
# recode 0/1 to be nongrad, grad.
yvar <- sea_data[train_idx, "ontime_grad"] # save only training observations
yvar <- ifelse(yvar == 1, "grad", "nongrad")
yvar <- factor(yvar)
# On a standard desktop/laptop it can be necessary to decrease the sample size
# to train in a reasonable amount of time. For the prototype and getting feedback
# it's a good idea to stick with a reasonable sample size of under 30,000 rows.
# Let's do that here:
train_idx_small <- sample(1:nrow(preds), 2e4)
# Caret has a number of complex options, you can read about under ?trainControl
# Here we set some sensible defaults
example_control <- trainControl(
method = "cv", # we cross-validate our model to avoid overfitting
classProbs = TRUE,  # we want to be able to predict probabilities, not just binary outcomes
returnData = TRUE, # we want to store the model data to allow for postestimation
summaryFunction = twoClassSummary, # we want to use the prSummary for better two-class accuracy measures
trim = TRUE, # we want to reduce the size of the final model object to save RAM
savePredictions = "final", # we want to store the predictions from the final model
returnResamp = "final", # we want to store the resampling code to directly compare other methods
allowParallel = TRUE # we want to use all the processors on our computer if possible
)
# This will take quite some time:
set.seed(2532) # set seed so models are comparable
rpart_model <- train(
y = yvar[train_idx_small], # specify the outcome, here subset
# to the smaller number of observations
x = preds[train_idx_small, ], # specify the matrix of predictors, again subset
method = "rpart",  # choose the algorithm - here a regression tree
tuneLength = 24, # choose how many values of tuning parameters to test
trControl = example_control, # set up the conditions for the test (defined above)
metric = "ROC" # select the metric to choose the best model based on
)
set.seed(2532)
# Repeat above but just change the `method` argument to nb for naiveBayes
nb_model <- train(y = yvar[train_idx_small],
x = preds[train_idx_small,],
method = "nb", tuneLength = 24,
trControl = example_control,
metric = "ROC")
summary(nb_model)
nb_model
# Take advantage of all the processing power on your machine
library(doFuture)
plan(multiprocess(workers = 4)) # define the number of cpus to use
registerDoFuture() # register them with R
# Caret really really really likes if you do binary classification that you
# code the variables as factors with alphabetical labels. In this case, we
# recode 0/1 to be nongrad, grad.
yvar <- sea_data[train_idx, "ontime_grad"] # save only training observations
yvar <- ifelse(yvar == 1, "grad", "nongrad")
yvar <- factor(yvar)
# On a standard desktop/laptop it can be necessary to decrease the sample size
# to train in a reasonable amount of time. For the prototype and getting feedback
# it's a good idea to stick with a reasonable sample size of under 30,000 rows.
# Let's do that here:
train_idx_small <- sample(1:nrow(preds), 3e4)
# Caret has a number of complex options, you can read about under ?trainControl
# Here we set some sensible defaults
example_control <- trainControl(
method = "cv", # we cross-validate our model to avoid overfitting
classProbs = TRUE,  # we want to be able to predict probabilities, not just binary outcomes
returnData = TRUE, # we want to store the model data to allow for postestimation
summaryFunction = twoClassSummary, # we want to use the prSummary for better two-class accuracy measures
trim = TRUE, # we want to reduce the size of the final model object to save RAM
savePredictions = "final", # we want to store the predictions from the final model
returnResamp = "final", # we want to store the resampling code to directly compare other methods
allowParallel = TRUE # we want to use all the processors on our computer if possible
)
# This will take quite some time:
set.seed(2532) # set seed so models are comparable
rpart_model <- train(
y = yvar[train_idx_small], # specify the outcome, here subset
# to the smaller number of observations
x = preds[train_idx_small, ], # specify the matrix of predictors, again subset
method = "rpart",  # choose the algorithm - here a regression tree
tuneLength = 24, # choose how many values of tuning parameters to test
trControl = example_control, # set up the conditions for the test (defined above)
metric = "ROC" # select the metric to choose the best model based on
)
set.seed(2532)
# Repeat above but just change the `method` argument to nb for naiveBayes
nb_model <- train(y = yvar[train_idx_small],
x = preds[train_idx_small,],
method = "nb", tuneLength = 24,
trControl = example_control,
metric = "ROC")
# Take advantage of all the processing power on your machine
library(doFuture)
plan(multiprocess(workers = 4)) # define the number of cpus to use
registerDoFuture() # register them with R
# Caret really really really likes if you do binary classification that you
# code the variables as factors with alphabetical labels. In this case, we
# recode 0/1 to be nongrad, grad.
yvar <- sea_data[train_idx, "ontime_grad"] # save only training observations
yvar <- ifelse(yvar == 1, "grad", "nongrad")
yvar <- factor(yvar)
# On a standard desktop/laptop it can be necessary to decrease the sample size
# to train in a reasonable amount of time. For the prototype and getting feedback
# it's a good idea to stick with a reasonable sample size of under 20,000 rows.
# Let's do that here:
train_idx_small <- sample(1:nrow(preds), 2e4)
# Caret has a number of complex options, you can read about under ?trainControl
# Here we set some sensible defaults
example_control <- trainControl(
method = "cv", # we cross-validate our model to avoid overfitting
classProbs = TRUE,  # we want to be able to predict probabilities, not just binary outcomes
returnData = TRUE, # we want to store the model data to allow for postestimation
summaryFunction = twoClassSummary, # we want to use the prSummary for better two-class accuracy measures
trim = TRUE, # we want to reduce the size of the final model object to save RAM
savePredictions = "final", # we want to store the predictions from the final model
returnResamp = "final", # we want to store the resampling code to directly compare other methods
allowParallel = TRUE # we want to use all the processors on our computer if possible
)
# This will take quite some time:
set.seed(2532) # set seed so models are comparable
rpart_model <- train(
y = yvar[train_idx_small], # specify the outcome, here subset
# to the smaller number of observations
x = preds[train_idx_small, ], # specify the matrix of predictors, again subset
method = "rpart",  # choose the algorithm - here a regression tree
tuneLength = 24, # choose how many values of tuning parameters to test
trControl = example_control, # set up the conditions for the test (defined above)
metric = "ROC" # select the metric to choose the best model based on
)
set.seed(2532)
# Repeat above but just change the `method` argument to nb for naiveBayes
nb_model <- train(y = yvar[train_idx_small],
x = preds[train_idx_small,],
method = "rf", tuneLength = 6,
trControl = example_control,
metric = "ROC")
